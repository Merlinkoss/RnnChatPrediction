{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предсказываем ответ пользователя с помощью RNN (GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "import functools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка данных\n",
    "Данные хранятся в виде чатов:\n",
    "* Одна строка - одна реплика\n",
    "* Вопрос боту начинается с символа >\n",
    "* Ответ бота начинается с символа <\n",
    "* Чаты разделены строкой с текстом ===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Общее количество символов: 13694221\n",
      "Уникальных символов: 93\n"
     ]
    }
   ],
   "source": [
    "# Загружаем набор чатов на которых будем обучаться\n",
    "text = open('myMessages.txt', 'rb').read().decode(encoding='utf-8')\n",
    "print ('Общее количество символов: {}'.format(len(text)))\n",
    "\n",
    "# Составляем словарь символов\n",
    "vocab = sorted(set(text))\n",
    "print ('Уникальных символов: {}'.format(len(vocab)))\n",
    "\n",
    "# Функции для преобразования текста в массив чисел и обратно\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подгатавливаем обучающий датасет\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//seq_length\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /mnt/udata/anaconda/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Вход:   '> я вот что поставила\\n===\\n< ок\\n< откуда лиса?\\n> не згаю, бвла в телефоне. плохая?\\n< нет хорошая\\n> а '\n",
      "Выход: ' я вот что поставила\\n===\\n< ок\\n< откуда лиса?\\n> не згаю, бвла в телефоне. плохая?\\n< нет хорошая\\n> а ч'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Вход:  ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Выход:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
    "\n",
    "SHUFFLE_BUFFER_SIZE = 10000\n",
    "dataset = dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Размер используемого словаря\n",
    "vocab_size = len(vocab)\n",
    "# Размерности сети\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "rnn = functools.partial(tf.keras.layers.GRU, recurrent_activation='sigmoid')\n",
    "# Для ускорения работы на GPU можно использовать rnn = tf.keras.layers.CuDNNGRU, но такая сеть не может быть потом \n",
    "# использована для работы на CPU.\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "      rnn(rnn_units,\n",
    "          return_sequences=True,\n",
    "          recurrent_initializer='glorot_uniform',\n",
    "          stateful=True),\n",
    "      tf.keras.layers.Dense(vocab_size)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (64, None, 256)           23808     \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (64, None, 1024)          3935232   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (64, None, 93)            95325     \n",
      "=================================================================\n",
      "Total params: 4,054,365\n",
      "Trainable params: 4,054,365\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "model.compile(\n",
    "    optimizer = tf.train.AdamOptimizer(),\n",
    "    loss = loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настройка сохранения результатов\n",
    "checkpoint_dir = 'training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2138/2139 [============================>.] - ETA: 0s - loss: 1.7443WARNING:tensorflow:From /mnt/udata/anaconda/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py:1436: update_checkpoint_state (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.train.CheckpointManager to manage checkpoints rather than manually editing the Checkpoint proto.\n",
      "2139/2139 [==============================] - 386s 181ms/step - loss: 1.7442\n",
      "Epoch 2/5\n",
      "2139/2139 [==============================] - 385s 180ms/step - loss: 1.4516\n",
      "Epoch 3/5\n",
      "2139/2139 [==============================] - 389s 182ms/step - loss: 1.4044\n",
      "Epoch 4/5\n",
      "2139/2139 [==============================] - 387s 181ms/step - loss: 1.3796\n",
      "Epoch 5/5\n",
      "2139/2139 [==============================] - 384s 179ms/step - loss: 1.3647\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset.repeat(), epochs=5, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка обученой модели из файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (1, None, 256)            23808     \n",
      "_________________________________________________________________\n",
      "gru_6 (GRU)                  (1, None, 1024)           3935232   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (1, None, 93)             95325     \n",
      "=================================================================\n",
      "Total params: 4,054,365\n",
      "Trainable params: 4,054,365\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Загружаем модель из checkpoint'а, при этом используем batch_size размера 1, чтобы можно было использовать \n",
    "# модель в режиме чата\n",
    "tf.train.latest_checkpoint(checkpoint_dir)\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование результатов работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, oneString, temperature):\n",
    "  # Максимальное количество генерируемых символов\n",
    "  num_generate = 1000\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  text_generated = []\n",
    "\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "    predictions = model(input_eval)\n",
    "    predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "    if temperature > 0:\n",
    "        predictions = predictions / temperature\n",
    "    predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "    input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    c = idx2char[predicted_id]\n",
    "    text_generated.append(c)\n",
    "    if c == '\\n' and oneString:\n",
    "        break\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Автоматическое завершение чатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===\n",
      "> куда завтра сходим?\n",
      "< в магазин\n",
      "\n",
      "===\n",
      "> когда завтра сходим?\n",
      "< в 8\n",
      "\n",
      "===\n",
      "> где ты сейчас?\n",
      "< в метро\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, \"===\\n> куда завтра сходим?\\n< в \", True, 0.001))\n",
    "print(generate_text(model, \"===\\n> когда завтра сходим?\\n< в \", True, 0.001))\n",
    "print(generate_text(model, \"===\\n> где ты сейчас?\\n< в \", True, 0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Чат-бот"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> привет\n",
      "< почему ты так подумал?\n",
      "\n",
      "> когда домой идешь?\n",
      "< да\n",
      "\n",
      "> пока\n",
      "< ну давай тогда позвоню\n",
      "\n",
      "> \n"
     ]
    }
   ],
   "source": [
    "dialog = u\"===\\n\"\n",
    "while(True):\n",
    "  rq = input(\"> \")\n",
    "  if rq == '':\n",
    "        break;\n",
    "  dialog += \"> \" + rq + \"\\n< \"\n",
    "  \n",
    "  fullAns = generate_text(model, start_string=dialog, temperature=0.3, oneString = True)\n",
    "  shortAns = fullAns[len(dialog):]\n",
    "  print(\"< \" + shortAns)\n",
    "  dialog = fullAns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Генератор чатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===\n",
      "> как дела?\n",
      "< да ничего не понял\n",
      "> ну так и не поняла\n",
      "> ну как после поездки на работу?\n",
      "< нет\n",
      "> почему?\n",
      "< ну вот так все понятно было\n",
      "> ну вот так и не поедем\n",
      "< ну вот ты скажешь что надо за ним поехать\n",
      "< ну вот просто потом посмотрим\n",
      "< надо было не поехать на дачу\n",
      "< но не понятно как я так понял\n",
      "> ну так и поедем\n",
      "< ну вот приедем как с дачей в самолете\n",
      "> ну вот скажи как ты сказал, что он не поедет на дачу\n",
      "< ну там не получается подумать\n",
      "> ну вот смотри, так и не понятно как его тебе не покупать\n",
      "< ну почему не понятно\n",
      "> ну вот посмотри как пойдем\n",
      "< ну в смысле не помогло\n",
      "> ну вот смотри, он же все против выгодно получить :(\n",
      "< ну я понимаю что надо было покупать\n",
      "> ну вот прямо сейчас выйдешь и поедем на дачу\n",
      "< ну да, но не помню\n",
      "> ну вот смотри, там все по сути он его не покупает\n",
      "> ну и что?\n",
      "< ну и в самом деле не получается\n",
      "> ну вот смотри, почему ты не поедешь в какой-то проблему с кроватью по дороге после поездки в магазине на работу на дачу и все после работы в комнате поставить н\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, \"===\", False, 0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
