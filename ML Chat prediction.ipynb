{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предсказываем ответ пользователя с помощью RNN (GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "import functools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка данных\n",
    "Данные хранятся в виде чатов:\n",
    "* Одна строка - одна реплика\n",
    "* Вопрос боту начинается с символа >\n",
    "* Ответ бота начинается с символа <\n",
    "* Чаты разделены строкой с текстом ===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Общее количество символов: 13694221\n",
      "Уникальных символов: 93\n"
     ]
    }
   ],
   "source": [
    "# Загружаем набор чатов на которых будем обучаться\n",
    "text = open('myMessages.txt', 'rb').read().decode(encoding='utf-8')\n",
    "print ('Общее количество символов: {}'.format(len(text)))\n",
    "\n",
    "# Составляем словарь символов\n",
    "vocab = sorted(set(text))\n",
    "print ('Уникальных символов: {}'.format(len(vocab)))\n",
    "\n",
    "# Функции для преобразования текста в массив чисел и обратно\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подгатавливаем обучающий датасет\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//seq_length\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /mnt/udata/anaconda/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Вход:   '> я вот что поставила\\n===\\n< ок\\n< откуда лиса?\\n> не згаю, бвла в телефоне. плохая?\\n< нет хорошая\\n> а '\n",
      "Выход: ' я вот что поставила\\n===\\n< ок\\n< откуда лиса?\\n> не згаю, бвла в телефоне. плохая?\\n< нет хорошая\\n> а ч'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Вход:  ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Выход:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
    "\n",
    "SHUFFLE_BUFFER_SIZE = 10000\n",
    "dataset = dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Размер используемого словаря\n",
    "vocab_size = len(vocab)\n",
    "# Размерности сети\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "\n",
    "# Здесь можно попробовать различные варианты архитектуры сети. На текущий момент лучший вариант у LSTM.\n",
    "\n",
    "#rnn = functools.partial(tf.keras.layers.GRU, recurrent_activation='sigmoid')\n",
    "rnn = tf.keras.layers.CuDNNLSTM\n",
    "#rnn = tf.keras.layers.CuDNNGRU\n",
    "\n",
    "# Для ускорения работы на GPU можно использовать rnn = tf.keras.layers.CuDNNGRU, но такая сеть не может быть потом \n",
    "# использована для работы на CPU.\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "      rnn(rnn_units,\n",
    "          return_sequences=True,\n",
    "          recurrent_initializer='glorot_uniform',\n",
    "          stateful=True),\n",
    "      tf.keras.layers.Dense(vocab_size)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           23808     \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm (CuDNNLSTM)       (64, None, 1024)          5251072   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 93)            95325     \n",
      "=================================================================\n",
      "Total params: 5,370,205\n",
      "Trainable params: 5,370,205\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "model.compile(\n",
    "    optimizer = tf.train.AdamOptimizer(),\n",
    "    loss = loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настройка сохранения результатов\n",
    "checkpoint_dir = 'training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2138/2139 [============================>.] - ETA: 0s - loss: 1.7243WARNING:tensorflow:From /mnt/udata/anaconda/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py:1436: update_checkpoint_state (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.train.CheckpointManager to manage checkpoints rather than manually editing the Checkpoint proto.\n",
      "2139/2139 [==============================] - 172s 80ms/step - loss: 1.7241\n",
      "Epoch 2/5\n",
      "2139/2139 [==============================] - 175s 82ms/step - loss: 1.4423\n",
      "Epoch 3/5\n",
      "2139/2139 [==============================] - 171s 80ms/step - loss: 1.3940\n",
      "Epoch 4/5\n",
      "2139/2139 [==============================] - 173s 81ms/step - loss: 1.3664\n",
      "Epoch 5/5\n",
      "2139/2139 [==============================] - 173s 81ms/step - loss: 1.3470\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset.repeat(), epochs=5, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка обученой модели из файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (1, None, 256)            23808     \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)     (1, None, 1024)           5251072   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (1, None, 93)             95325     \n",
      "=================================================================\n",
      "Total params: 5,370,205\n",
      "Trainable params: 5,370,205\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Загружаем модель из checkpoint'а, при этом используем batch_size размера 1, чтобы можно было использовать \n",
    "# модель в режиме чата\n",
    "tf.train.latest_checkpoint(checkpoint_dir)\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование результатов работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, oneString, temperature):\n",
    "  # Максимальное количество генерируемых символов\n",
    "  num_generate = 200\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  text_generated = []\n",
    "\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "    predictions = model(input_eval)\n",
    "    predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "    if temperature > 0:\n",
    "        predictions = predictions / temperature\n",
    "    predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "    input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    c = idx2char[predicted_id]\n",
    "    text_generated.append(c)\n",
    "    if c == '\\n' and oneString:\n",
    "        break\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Автоматическое завершение чатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===\n",
      "> куда завтра сходим?\n",
      "< в магазин\n",
      "\n",
      "===\n",
      "> во сколько?\n",
      "< в 8 начал\n",
      "\n",
      "===\n",
      "> где ты сейчас?\n",
      "< в метро\n",
      "\n",
      "===\n",
      "> где ключи от машины?\n",
      "< в машине\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, \"===\\n> куда завтра сходим?\\n< в \", True, 0.001))\n",
    "print(generate_text(model, \"===\\n> во сколько?\\n< в \", True, 0.001))\n",
    "print(generate_text(model, \"===\\n> где ты сейчас?\\n< в \", True, 0.001))\n",
    "print(generate_text(model, \"===\\n> где ключи от машины?\\n< в \", True, 0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Чат-бот"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> ты сейчас где?\n",
      "< в метро\n",
      "\n",
      "> домой едешь?\n",
      "< нет\n",
      "\n",
      "> а куда?\n",
      "< на дачу\n",
      "\n",
      "> баг поправил?\n",
      "< да\n",
      "\n",
      "> отлично!\n",
      "< ну вот посмотрим\n",
      "\n",
      "> \n"
     ]
    }
   ],
   "source": [
    "dialog = u\"===\\n\"\n",
    "while(True):\n",
    "  rq = input(\"> \")\n",
    "  if rq == '':\n",
    "        break;\n",
    "  dialog += \"> \" + rq + \"\\n< \"\n",
    "  \n",
    "  fullAns = generate_text(model, start_string=dialog, temperature=0.1, oneString = True)\n",
    "  shortAns = fullAns[len(dialog):]\n",
    "  print(\"< \" + shortAns)\n",
    "  dialog = fullAns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Генератор чатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====\n",
      "> как дела?\n",
      "< норм\n",
      "===\n",
      "> как дела?\n",
      "< да ничего\n",
      "> ты где?\n",
      "< в метро\n",
      "> а что там?\n",
      "< ну вот подумали в поликлинику\n",
      "> а ты как?\n",
      "< пока нет\n",
      "< не знаю\n",
      "< надо посмотреть\n",
      "> а сейчас он сказал?\n",
      "< нет\n",
      "> ну вот как ты сегодня?\n",
      "< ну пока не пойду\n",
      "> ну вот ты там поедешь на дачу?\n",
      "< нет\n",
      "< собираюсь завтра с дедушкой и пойду\n",
      "> ну вот посмотрим\n",
      "> ну вот смотри какой с помощью калининграда\n",
      "> ну вот почему не поймешь, что ты так думаешь, что вот подумал, что ты не поедешь :(\n",
      "< ну вот ты так получил себя чувствую, что они там получается в доме как раз на полу в области до 10 лет\n",
      "< ну да, ну вот почему не будет до этого в него надо было сказать что это тоже самое время получается\n",
      "> ну, вот ты мне позвонил, он не помнится, что он не помогло было бы сделать с собой воду и спросила про нас на полчаса\n",
      "< ну да\n",
      "< ну надо было сказать что не получилось\n",
      "> ну почему не поймут?\n",
      "< ну я понимаю, что так не будет\n",
      "> ну почему не надо?\n",
      "> ну вот и пойду\n",
      "> не пойду\n",
      "< ну просто спросил, надо позвонить\n",
      "< сейчас пойду\n",
      "> ну\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, \"===\", False, 0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
